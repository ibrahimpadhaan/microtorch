# Tokenizer
This is a little tokenizer
bigram makes bigram from encoded text
frequency returns a list of int which tells how many times bigram appeared in whole text
make tokens make dictionary of len< vocabsize giving first places in dictionary to frequent bigrams
merge replaces raw bytes with tokens of dictionary
